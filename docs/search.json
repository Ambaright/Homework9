[
  {
    "objectID": "Seoul Bike Data Model Fitting.html",
    "href": "Seoul Bike Data Model Fitting.html",
    "title": "Seoul Bike Data Modeling",
    "section": "",
    "text": "We will use a dataset from the UCI Machine Learning Repository. This data set is about bike sharing rentals and is available at the assignment link. You can learn more about the data here. The data is available at https://www4.stat.ncsu.edu/~online/datasets/SeoulBikeData.csv\nThe data description describes the following variables:\n• Date : day/month/year • Rented Bike count - Count of bikes rented at each hour • Hour - Hour of the day • Temperature-Temperature in Celsius • Humidity - % • Windspeed - m/s • Visibility - 10m • Dew point temperature - Celsius • Solar radiation - MJ/m2 • Rainfall - mm • Snowfall - cm • Seasons - Winter, Spring, Summer, Autumn • Holiday - Holiday/No holiday • Functional Day - NoFunc(Non Functional Hours), Fun(Functional hours)"
  },
  {
    "objectID": "Seoul Bike Data Model Fitting.html#data",
    "href": "Seoul Bike Data Model Fitting.html#data",
    "title": "Seoul Bike Data Modeling",
    "section": "",
    "text": "We will use a dataset from the UCI Machine Learning Repository. This data set is about bike sharing rentals and is available at the assignment link. You can learn more about the data here. The data is available at https://www4.stat.ncsu.edu/~online/datasets/SeoulBikeData.csv\nThe data description describes the following variables:\n• Date : day/month/year • Rented Bike count - Count of bikes rented at each hour • Hour - Hour of the day • Temperature-Temperature in Celsius • Humidity - % • Windspeed - m/s • Visibility - 10m • Dew point temperature - Celsius • Solar radiation - MJ/m2 • Rainfall - mm • Snowfall - cm • Seasons - Winter, Spring, Summer, Autumn • Holiday - Holiday/No holiday • Functional Day - NoFunc(Non Functional Hours), Fun(Functional hours)"
  },
  {
    "objectID": "Seoul Bike Data Model Fitting.html#reading-data",
    "href": "Seoul Bike Data Model Fitting.html#reading-data",
    "title": "Seoul Bike Data Modeling",
    "section": "Reading Data",
    "text": "Reading Data\nBefore we can work with the data, we need to read it in!\n\nlibrary(tidyverse)\n\nWarning: package 'tidyverse' was built under R version 4.3.3\n\n\nWarning: package 'ggplot2' was built under R version 4.3.3\n\n\nWarning: package 'tidyr' was built under R version 4.3.3\n\n\nWarning: package 'stringr' was built under R version 4.3.3\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(tidymodels)\n\nWarning: package 'tidymodels' was built under R version 4.3.3\n\n\n── Attaching packages ────────────────────────────────────── tidymodels 1.2.0 ──\n✔ broom        1.0.6     ✔ rsample      1.2.1\n✔ dials        1.3.0     ✔ tune         1.2.1\n✔ infer        1.0.7     ✔ workflows    1.1.4\n✔ modeldata    1.4.0     ✔ workflowsets 1.1.0\n✔ parsnip      1.2.1     ✔ yardstick    1.3.1\n✔ recipes      1.1.0     \n\n\nWarning: package 'broom' was built under R version 4.3.3\n\n\nWarning: package 'dials' was built under R version 4.3.3\n\n\nWarning: package 'infer' was built under R version 4.3.3\n\n\nWarning: package 'modeldata' was built under R version 4.3.3\n\n\nWarning: package 'parsnip' was built under R version 4.3.3\n\n\nWarning: package 'recipes' was built under R version 4.3.3\n\n\nWarning: package 'rsample' was built under R version 4.3.3\n\n\nWarning: package 'tune' was built under R version 4.3.3\n\n\nWarning: package 'workflows' was built under R version 4.3.3\n\n\nWarning: package 'workflowsets' was built under R version 4.3.3\n\n\nWarning: package 'yardstick' was built under R version 4.3.3\n\n\n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ scales::discard() masks purrr::discard()\n✖ dplyr::filter()   masks stats::filter()\n✖ recipes::fixed()  masks stringr::fixed()\n✖ dplyr::lag()      masks stats::lag()\n✖ yardstick::spec() masks readr::spec()\n✖ recipes::step()   masks stats::step()\n• Learn how to get started at https://www.tidymodels.org/start/\n\nbike_data &lt;- read_csv(\"https://www4.stat.ncsu.edu/~online/datasets/SeoulBikeData.csv\",\n                      local = locale(encoding = \"latin1\"))\n\nRows: 8760 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (4): Date, Seasons, Holiday, Functioning Day\ndbl (10): Rented Bike Count, Hour, Temperature(°C), Humidity(%), Wind speed ...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nbike_data\n\n# A tibble: 8,760 × 14\n   Date       `Rented Bike Count`  Hour `Temperature(°C)` `Humidity(%)`\n   &lt;chr&gt;                    &lt;dbl&gt; &lt;dbl&gt;             &lt;dbl&gt;         &lt;dbl&gt;\n 1 01/12/2017                 254     0              -5.2            37\n 2 01/12/2017                 204     1              -5.5            38\n 3 01/12/2017                 173     2              -6              39\n 4 01/12/2017                 107     3              -6.2            40\n 5 01/12/2017                  78     4              -6              36\n 6 01/12/2017                 100     5              -6.4            37\n 7 01/12/2017                 181     6              -6.6            35\n 8 01/12/2017                 460     7              -7.4            38\n 9 01/12/2017                 930     8              -7.6            37\n10 01/12/2017                 490     9              -6.5            27\n# ℹ 8,750 more rows\n# ℹ 9 more variables: `Wind speed (m/s)` &lt;dbl&gt;, `Visibility (10m)` &lt;dbl&gt;,\n#   `Dew point temperature(°C)` &lt;dbl&gt;, `Solar Radiation (MJ/m2)` &lt;dbl&gt;,\n#   `Rainfall(mm)` &lt;dbl&gt;, `Snowfall (cm)` &lt;dbl&gt;, Seasons &lt;chr&gt;, Holiday &lt;chr&gt;,\n#   `Functioning Day` &lt;chr&gt;\n\n\nWe also need to have some necessary data cleaning to have a workable data set for our modeling analysis.\n\n# Make Date a date variable\nbike_data &lt;- bike_data |&gt;\n  mutate(date = lubridate::dmy(Date)) |&gt;\n  select(-Date)\n\n# Make categorical predictors factors\nbike_data &lt;- bike_data |&gt;\n  mutate(seasons = factor(Seasons),\n  holiday = factor(Holiday),\n  fn_day = factor(`Functioning Day`)) |&gt;\n  select(-Seasons, -Holiday, -`Functioning Day`)\n\n# Rename variables for ease of use\nbike_data &lt;- bike_data |&gt;\n  rename('bike_count' = `Rented Bike Count`,\n        'hour' = \"Hour\",\n        \"temp\" = `Temperature(°C)`,\n        \"wind_speed\" = `Wind speed (m/s)`,\n        \"humidity\" = `Humidity(%)`,\n        \"vis\" = `Visibility (10m)`,\n        \"dew_point_temp\" = `Dew point temperature(°C)`,\n        \"solar_radiation\" = `Solar Radiation (MJ/m2)`,\n        \"rainfall\" = \"Rainfall(mm)\",\n        \"snowfall\" = `Snowfall (cm)`)\n\n# Filter out out of commission days\nbike_data &lt;- bike_data |&gt;\n  filter(fn_day == \"Yes\") |&gt;\n  select(-fn_day)\n\nbike_data\n\n# A tibble: 8,465 × 13\n   bike_count  hour  temp humidity wind_speed   vis dew_point_temp\n        &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;      &lt;dbl&gt; &lt;dbl&gt;          &lt;dbl&gt;\n 1        254     0  -5.2       37        2.2  2000          -17.6\n 2        204     1  -5.5       38        0.8  2000          -17.6\n 3        173     2  -6         39        1    2000          -17.7\n 4        107     3  -6.2       40        0.9  2000          -17.6\n 5         78     4  -6         36        2.3  2000          -18.6\n 6        100     5  -6.4       37        1.5  2000          -18.7\n 7        181     6  -6.6       35        1.3  2000          -19.5\n 8        460     7  -7.4       38        0.9  2000          -19.3\n 9        930     8  -7.6       37        1.1  2000          -19.8\n10        490     9  -6.5       27        0.5  1928          -22.4\n# ℹ 8,455 more rows\n# ℹ 6 more variables: solar_radiation &lt;dbl&gt;, rainfall &lt;dbl&gt;, snowfall &lt;dbl&gt;,\n#   date &lt;date&gt;, seasons &lt;fct&gt;, holiday &lt;fct&gt;\n\n\nWe note that since we are continuing from the last assignment, we can omit the EDA here. However, for more detail on EDA performed reference Homework 8."
  },
  {
    "objectID": "Seoul Bike Data Model Fitting.html#split-the-data",
    "href": "Seoul Bike Data Model Fitting.html#split-the-data",
    "title": "Seoul Bike Data Modeling",
    "section": "Split the Data",
    "text": "Split the Data\n• Use functions from tidymodels to split the data into a training and test set (75/25 split). Use the strata argument to stratify the split on the seasons.\n• On the training set, create a 10 fold CV split\n\nset.seed(11)\nbike_split &lt;- initial_split(bike_data, prop = 0.75, strata = seasons)\nbike_train &lt;- training(bike_split)\nbike_test &lt;- testing(bike_split)\nbike_10_fold &lt;- vfold_cv(bike_train, 10)"
  },
  {
    "objectID": "Seoul Bike Data Model Fitting.html#fitting-mlr-models",
    "href": "Seoul Bike Data Model Fitting.html#fitting-mlr-models",
    "title": "Seoul Bike Data Modeling",
    "section": "Fitting MLR Models",
    "text": "Fitting MLR Models\nFor the 1st recipe:\n• Let’s ignore the date variable (so we’ll need to remove that or give it a different ID) but use it to create a weekday/weekend (factor) variable. (See step 2 of the shinymodels tutorial! You can use 9 step_date() then step_mutate() with an factor(if_else(…)) to create the variable. I then had to remove the intermediate variable created.)\n• Let’s standardize the numeric variables since their scales are pretty different.\n• Let’s create dummy variables for the seasons, holiday, and our new day type variable\n\nMLR_rec1 &lt;- recipe(bike_count ~ ., data = bike_train) |&gt;\n  step_date(date, features = \"dow\") |&gt;\n  step_mutate(day_type = factor(if_else(date_dow %in% c(\"Sat\", \"Sun\"), \"Weekend\", \"Weekday\"))) |&gt;\n  step_rm(date, date_dow) |&gt;\n  step_dummy(seasons, holiday, day_type) |&gt;\n  step_normalize(all_numeric(), -bike_count)\n\nFor the 2nd recipe:\n• Do the same steps as above.\n• Add in interactions between seasons and holiday, seasons and temp, temp and rainfall. For the seasons interactions, you can use starts_with() to create the proper interactions.\n\nMLR_rec2 &lt;- MLR_rec1 |&gt;\n  step_interact(terms = ~starts_with(\"seasons\")*starts_with(\"holiday\") +\n  starts_with(\"seasons\")*temp +\n  temp*rainfall)\n\nFor the 3rd recipe:\n• Do the same as the 2nd recipe.\n• Add in quadratic terms for each numeric predictor\n\nMLR_rec3 &lt;- MLR_rec2 |&gt;\n  step_poly(temp,\n          wind_speed,\n          vis,\n          dew_point_temp,\n          solar_radiation,\n          rainfall,\n          snowfall,\n          degree = 2)\n\nNow set up linear model fit.\n\nMLR_spec &lt;- linear_reg() |&gt;\n  set_engine(\"lm\")\n\nFit the models using 10 fold CV and consider the training set CV error to choose a best model.\n\nMLR_CV_fit1 &lt;- workflow() |&gt;\n  add_recipe(MLR_rec1) |&gt;\n  add_model(MLR_spec) |&gt;\n  fit_resamples(bike_10_fold)\n\nMLR_CV_fit2 &lt;- workflow() |&gt;\n  add_recipe(MLR_rec2) |&gt;\n  add_model(MLR_spec) |&gt;\n  fit_resamples(bike_10_fold)\n\nMLR_CV_fit3 &lt;- workflow() |&gt;\n  add_recipe(MLR_rec3) |&gt;\n  add_model(MLR_spec) |&gt;\n  fit_resamples(bike_10_fold)\n\nGet metrics:\n\nrbind(MLR_CV_fit1 |&gt; collect_metrics(),\n      MLR_CV_fit2 |&gt; collect_metrics(),\n      MLR_CV_fit3 |&gt; collect_metrics())\n\n# A tibble: 6 × 6\n  .metric .estimator    mean     n std_err .config             \n  &lt;chr&gt;   &lt;chr&gt;        &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard   431.       10 5.92    Preprocessor1_Model1\n2 rsq     standard     0.544    10 0.00911 Preprocessor1_Model1\n3 rmse    standard   418.       10 4.87    Preprocessor1_Model1\n4 rsq     standard     0.571    10 0.00809 Preprocessor1_Model1\n5 rmse    standard   393.       10 5.60    Preprocessor1_Model1\n6 rsq     standard     0.620    10 0.00969 Preprocessor1_Model1\n\n\nThe last MLR model performs the best, and will be the one we fit to the entire training set to see how it predicts to the test set.\n\nMLR_final_wkf &lt;- workflow() |&gt;\n  add_recipe(MLR_rec3) |&gt;\n  add_model(MLR_spec)\n\nHowever, before we fit the best MLR model to the entire trianing set we will fit a few other models."
  },
  {
    "objectID": "Seoul Bike Data Model Fitting.html#fitting-a-tuned-lasso-model",
    "href": "Seoul Bike Data Model Fitting.html#fitting-a-tuned-lasso-model",
    "title": "Seoul Bike Data Modeling",
    "section": "Fitting a (tuned) LASSO model",
    "text": "Fitting a (tuned) LASSO model\nWe first need to build a LASSO recipe, which we can choose to be similar to our MLR_rec1.\n\nLASSO_rec &lt;- recipe(bike_count ~ ., data = bike_train) |&gt;\n  step_date(date, features = \"dow\") |&gt;\n  step_mutate(day_type = factor(if_else(date_dow %in% c(\"Sat\", \"Sun\"), \"Weekend\", \"Weekday\"))) |&gt;\n  step_rm(date, date_dow) |&gt;\n  step_dummy(seasons, holiday, day_type) |&gt;\n  step_normalize(all_numeric(), -bike_count)\n\nNow we need to specify a (tuned) LASSO model.\n\nLASSO_spec &lt;- linear_reg(penalty = tune(), mixture = 1) |&gt;\n  set_engine(\"glmnet\")\n\nNow we need our workflow.\n\nLASSO_wkf &lt;- workflow() |&gt;\n  add_recipe(LASSO_rec) |&gt;\n  add_model(LASSO_spec)\n\nNow we need to fit the model with tune_grid() and grid_regular().\n\nLASSO_grid &lt;- LASSO_wkf |&gt;\n  tune_grid(resamples = bike_10_fold,\n            grid = grid_regular(penalty(), levels = 200))\n\nWarning: package 'glmnet' was built under R version 4.3.3\n\n\nWe recall that the LASSO_grid gives us a tibble with 400 metrics, but we may want these metrics computed across the folds for each of the 200 values of the tuning parameter.\n\nLASSO_grid |&gt;\n  collect_metrics()\n\n# A tibble: 400 × 7\n    penalty .metric .estimator    mean     n std_err .config               \n      &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;        &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;                 \n 1 1   e-10 rmse    standard   431.       10 5.94    Preprocessor1_Model001\n 2 1   e-10 rsq     standard     0.544    10 0.00915 Preprocessor1_Model001\n 3 1.12e-10 rmse    standard   431.       10 5.94    Preprocessor1_Model002\n 4 1.12e-10 rsq     standard     0.544    10 0.00915 Preprocessor1_Model002\n 5 1.26e-10 rmse    standard   431.       10 5.94    Preprocessor1_Model003\n 6 1.26e-10 rsq     standard     0.544    10 0.00915 Preprocessor1_Model003\n 7 1.41e-10 rmse    standard   431.       10 5.94    Preprocessor1_Model004\n 8 1.41e-10 rsq     standard     0.544    10 0.00915 Preprocessor1_Model004\n 9 1.59e-10 rmse    standard   431.       10 5.94    Preprocessor1_Model005\n10 1.59e-10 rsq     standard     0.544    10 0.00915 Preprocessor1_Model005\n# ℹ 390 more rows\n\n\nWe can then pull out the “best” model with select_best() and finalize_workflow().\n\nlowest_rmse_LASSO &lt;- LASSO_grid |&gt;\n  select_best(metric = \"rmse\")\nlowest_rmse_LASSO\n\n# A tibble: 1 × 2\n       penalty .config               \n         &lt;dbl&gt; &lt;chr&gt;                 \n1 0.0000000001 Preprocessor1_Model001\n\n\n\nLASSO_wkf |&gt;\n  finalize_workflow(lowest_rmse_LASSO)\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: linear_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n5 Recipe Steps\n\n• step_date()\n• step_mutate()\n• step_rm()\n• step_dummy()\n• step_normalize()\n\n── Model ───────────────────────────────────────────────────────────────────────\nLinear Regression Model Specification (regression)\n\nMain Arguments:\n  penalty = 1e-10\n  mixture = 1\n\nComputational engine: glmnet \n\n\nWe’ll hold off fitting this to the entire training set."
  },
  {
    "objectID": "Seoul Bike Data Model Fitting.html#fitting-a-tuned-regression-tree-model",
    "href": "Seoul Bike Data Model Fitting.html#fitting-a-tuned-regression-tree-model",
    "title": "Seoul Bike Data Modeling",
    "section": "Fitting a (tuned) Regression Tree Model",
    "text": "Fitting a (tuned) Regression Tree Model\nWe first need to create our regression tree recipe. It’s important to remember that our tree based models inherently include interactions, so we can again use a similar recipe to MLR_rec.\n\ntree_rec &lt;- recipe(bike_count ~ ., data = bike_train) |&gt;\n  step_date(date, features = \"dow\") |&gt;\n  step_mutate(day_type = factor(if_else(date_dow %in% c(\"Sat\", \"Sun\"), \"Weekend\", \"Weekday\"))) |&gt;\n  step_rm(date, date_dow) |&gt;\n  step_dummy(seasons, holiday, day_type) |&gt;\n  step_normalize(all_numeric(), -bike_count)\n\nWe now need to specify our model.\n\n# tune tree_depth and cost_complexity\ntree_spec &lt;- decision_tree(tree_depth = tune(),\n                           min_n = 20,\n                           cost_complexity = tune()) |&gt;\n  set_engine(\"rpart\") |&gt;\n  set_mode(\"regression\")\n\nNow we can create our workflow.\n\ntree_wkf &lt;- workflow() |&gt;\n  add_recipe(tree_rec) |&gt;\n  add_model(tree_spec)\n\nNow we can use CV to select our tuning parameters.\n\ntree_grid &lt;- grid_regular(cost_complexity(),\n                          tree_depth(),\n                          levels = c(10,5))\n\ntree_fits &lt;- tree_wkf |&gt;\n  tune_grid(resamples = bike_10_fold,\n            grid = tree_grid)\n\nWe can then collect the metric from our fitted regression tree.\n\ntree_fits |&gt;\n  collect_metrics()\n\n# A tibble: 100 × 8\n   cost_complexity tree_depth .metric .estimator    mean     n std_err .config  \n             &lt;dbl&gt;      &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;        &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;    \n 1    0.0000000001          1 rmse    standard   542.       10 7.56    Preproce…\n 2    0.0000000001          1 rsq     standard     0.279    10 0.00989 Preproce…\n 3    0.000000001           1 rmse    standard   542.       10 7.56    Preproce…\n 4    0.000000001           1 rsq     standard     0.279    10 0.00989 Preproce…\n 5    0.00000001            1 rmse    standard   542.       10 7.56    Preproce…\n 6    0.00000001            1 rsq     standard     0.279    10 0.00989 Preproce…\n 7    0.0000001             1 rmse    standard   542.       10 7.56    Preproce…\n 8    0.0000001             1 rsq     standard     0.279    10 0.00989 Preproce…\n 9    0.000001              1 rmse    standard   542.       10 7.56    Preproce…\n10    0.000001              1 rsq     standard     0.279    10 0.00989 Preproce…\n# ℹ 90 more rows\n\n\nNow we can select the best tree model parameters.\n\ntree_best_params &lt;- select_best(tree_fits, metric = \"rmse\")\ntree_best_params\n\n# A tibble: 1 × 3\n  cost_complexity tree_depth .config              \n            &lt;dbl&gt;      &lt;int&gt; &lt;chr&gt;                \n1        0.000001         15 Preprocessor1_Model45\n\n\nNow we finalize the workflow.\n\ntree_final_wkf &lt;- tree_wkf |&gt;\n  finalize_workflow(tree_best_params)\n\nWe’ll wait to refit the entire training data set on these tuning parameters."
  },
  {
    "objectID": "Seoul Bike Data Model Fitting.html#fitting-a-tuned-bagged-tree-model",
    "href": "Seoul Bike Data Model Fitting.html#fitting-a-tuned-bagged-tree-model",
    "title": "Seoul Bike Data Modeling",
    "section": "Fitting a (tuned) Bagged Tree Model",
    "text": "Fitting a (tuned) Bagged Tree Model\nWe’re again going to create our bagged tree recipe, however, we can just use our MLR_rec1 recipe.\n\nbag_rec &lt;- MLR_rec1\n\nWe then need to specify our bagged model with cost_complexity being a tuning parameter.\n\nbag_spec &lt;- bag_tree(tree_depth = 5, min_n = 10, cost_complexity = tune()) |&gt;\n  set_engine(\"rpart\") |&gt;\n  set_mode(\"regression\")\n\nWe then need to create our workflow.\n\nlibrary(baguette)\n\nWarning: package 'baguette' was built under R version 4.3.3\n\nbag_wkf &lt;- workflow() |&gt;\n  add_recipe(bag_rec) |&gt;\n  add_model(bag_spec)\n\nWe now need to fit our CV folds.\n\nbag_fit &lt;- bag_wkf |&gt;\n  tune_grid(resamples = bike_10_fold,\n            grid = grid_regular(cost_complexity(),\n                                levels = 15))\n\nWe need to collect our metrics.\n\nbag_fit |&gt;\n  collect_metrics() |&gt;\n  filter(.metric == \"rmse\") |&gt;\n  arrange(mean)\n\n# A tibble: 15 × 7\n   cost_complexity .metric .estimator  mean     n std_err .config              \n             &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;                \n 1        1.39e- 5 rmse    standard    296.    10    3.77 Preprocessor1_Model09\n 2        4.39e-10 rmse    standard    297.    10    5.95 Preprocessor1_Model02\n 3        2.68e- 4 rmse    standard    297.    10    2.95 Preprocessor1_Model11\n 4        3.73e- 8 rmse    standard    298.    10    4.75 Preprocessor1_Model05\n 5        6.11e- 5 rmse    standard    298.    10    4.07 Preprocessor1_Model10\n 6        1.64e- 7 rmse    standard    298.    10    4.16 Preprocessor1_Model06\n 7        1.93e- 9 rmse    standard    299.    10    4.86 Preprocessor1_Model03\n 8        7.20e- 7 rmse    standard    299.    10    5.38 Preprocessor1_Model07\n 9        1   e-10 rmse    standard    299.    10    5.21 Preprocessor1_Model01\n10        8.48e- 9 rmse    standard    299.    10    4.61 Preprocessor1_Model04\n11        3.16e- 6 rmse    standard    300.    10    4.75 Preprocessor1_Model08\n12        1.18e- 3 rmse    standard    301.    10    4.81 Preprocessor1_Model12\n13        5.18e- 3 rmse    standard    311.    10    3.43 Preprocessor1_Model13\n14        2.28e- 2 rmse    standard    368.    10    4.78 Preprocessor1_Model14\n15        1   e- 1 rmse    standard    461.    10    5.51 Preprocessor1_Model15\n\n\nNow we can select the best tuning parameters.\n\nbag_best_params &lt;- select_best(bag_fit, metric = \"rmse\")\nbag_best_params\n\n# A tibble: 1 × 2\n  cost_complexity .config              \n            &lt;dbl&gt; &lt;chr&gt;                \n1       0.0000139 Preprocessor1_Model09\n\n\nAnd finalize the workflow.\n\nbag_final_wkf &lt;- bag_wkf |&gt;\n  finalize_workflow(bag_best_params)\n\nWe’ll wait to refit on the entire training set using this tuning parameter."
  },
  {
    "objectID": "Seoul Bike Data Model Fitting.html#fitting-a-tuned-random-forest-model",
    "href": "Seoul Bike Data Model Fitting.html#fitting-a-tuned-random-forest-model",
    "title": "Seoul Bike Data Modeling",
    "section": "Fitting a (tuned) Random Forest Model",
    "text": "Fitting a (tuned) Random Forest Model\nNow we want to create a random forest, again with the same MLR_rec1 recipe.\n\nrf_rec &lt;- MLR_rec1\n\nNow we need to specify our random forest model.\n\nrf_spec &lt;- rand_forest(mtry = tune()) |&gt;\n  set_engine(\"ranger\") |&gt;\n  set_mode(\"regression\")\n\nNow we set up our workflow.\n\nrf_wkf &lt;- workflow() |&gt;\n  add_recipe(rf_rec) |&gt;\n  add_model(rf_spec)\n\nNow we need to fit our CV folds.\n\nrf_fit &lt;- rf_wkf |&gt;\n  tune_grid(resamples = bike_10_fold,\n            grid = 7)\n\ni Creating pre-processing data to finalize unknown parameter: mtry\n\n\nNow we can look at our metrics across the folds.\n\nrf_fit |&gt;\n  collect_metrics() |&gt;\n  filter(.metric == \"rmse\") |&gt;\n  arrange(mean)\n\n# A tibble: 7 × 7\n   mtry .metric .estimator  mean     n std_err .config             \n  &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1    11 rmse    standard    182.    10    5.88 Preprocessor1_Model4\n2    10 rmse    standard    183.    10    6.00 Preprocessor1_Model7\n3    14 rmse    standard    184.    10    5.69 Preprocessor1_Model1\n4     7 rmse    standard    188.    10    5.86 Preprocessor1_Model2\n5     6 rmse    standard    191.    10    5.77 Preprocessor1_Model6\n6     4 rmse    standard    204.    10    5.50 Preprocessor1_Model5\n7     2 rmse    standard    267.    10    4.54 Preprocessor1_Model3\n\n\nNow we can select our best tuning parameter.\n\nrf_best_params &lt;- select_best(rf_fit, metric = \"rmse\")\nrf_best_params\n\n# A tibble: 1 × 2\n   mtry .config             \n  &lt;int&gt; &lt;chr&gt;               \n1    11 Preprocessor1_Model4\n\n\nNow we can finalize our workflow.\n\nrf_final_wkf &lt;- rf_wkf |&gt;\n  finalize_workflow(rf_best_params)"
  },
  {
    "objectID": "Seoul Bike Data Model Fitting.html#fit-best-models-on-the-entire-training-set",
    "href": "Seoul Bike Data Model Fitting.html#fit-best-models-on-the-entire-training-set",
    "title": "Seoul Bike Data Modeling",
    "section": "Fit Best Models on the Entire Training Set",
    "text": "Fit Best Models on the Entire Training Set\nEach of these models should be fit and tuned on the training set. You should take the best model from each family of models (best LASSO, best Regression tree,. . . ) and fit it to the entire training data set and see how it predicts on the test set. Include your best MLR model from the last homework here.\n• Compare all final models on the test set using both rmse and mae (mean absolute error)\nWe can first start with our best MLR model.\n\nfinal_MLR_fit &lt;- MLR_final_wkf |&gt;\n  last_fit(bike_split, metrics = metric_set(rmse, mae))\n\nfinal_MLR_fit |&gt;\n  collect_metrics()\n\n# A tibble: 2 × 4\n  .metric .estimator .estimate .config             \n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard        404. Preprocessor1_Model1\n2 mae     standard        297. Preprocessor1_Model1\n\n\nNow the best LASSO model.\n\nLASSO_final &lt;- LASSO_wkf |&gt;\n  finalize_workflow(lowest_rmse_LASSO)\n\nLASSO_final |&gt;\n  last_fit(bike_split, metrics = metric_set(rmse, mae)) |&gt;\n  collect_metrics()\n\n# A tibble: 2 × 4\n  .metric .estimator .estimate .config             \n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard        442. Preprocessor1_Model1\n2 mae     standard        328. Preprocessor1_Model1\n\n\nNow the best Regression Tree model.\n\ntree_final_fit &lt;- tree_final_wkf |&gt;\n  last_fit(bike_split, metrics = metric_set(rmse, mae))\n\ntree_final_fit |&gt;\n  collect_metrics()\n\n# A tibble: 2 × 4\n  .metric .estimator .estimate .config             \n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard        227. Preprocessor1_Model1\n2 mae     standard        140. Preprocessor1_Model1\n\n\nNow the best Bagged Tree model.\n\nbag_final_fit &lt;- bag_final_wkf |&gt;\n  last_fit(bike_split, metrics = metric_set(rmse, mae))\n\nbag_final_fit |&gt;\n  collect_metrics()\n\n# A tibble: 2 × 4\n  .metric .estimator .estimate .config             \n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard        300. Preprocessor1_Model1\n2 mae     standard        198. Preprocessor1_Model1\n\n\nNow the best Random Forest model.\n\nrf_final_fit &lt;- rf_final_wkf |&gt;\n  last_fit(bike_split, metrics = metric_set(rmse, mae))\n\nrf_final_fit |&gt;\n  collect_metrics()\n\n# A tibble: 2 × 4\n  .metric .estimator .estimate .config             \n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard        174. Preprocessor1_Model1\n2 mae     standard        107. Preprocessor1_Model1\n\n\nWhen we compare all five of our best models based on RMSE and MAE, we see that the random forest model performs the best on the test set and is our best model overall. We also see that the LASSO model performs the worst compared to the other models."
  },
  {
    "objectID": "Seoul Bike Data Model Fitting.html#summaries-of-the-best-models",
    "href": "Seoul Bike Data Model Fitting.html#summaries-of-the-best-models",
    "title": "Seoul Bike Data Modeling",
    "section": "Summaries of the Best Models",
    "text": "Summaries of the Best Models\nIn addition to determining the best overall model, we can extract the final model fits for each type and report a summary of the model\n– For the LASSO and MLR models, report the final coefficient tables\n\nfinal_MLR_fit |&gt;\n  extract_fit_parsnip() |&gt;\n  tidy()\n\n# A tibble: 29 × 5\n   term                                estimate std.error statistic   p.value\n   &lt;chr&gt;                                  &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n 1 (Intercept)                            803.      29.4      27.3  2.94e-155\n 2 hour                                   199.       5.45     36.4  1.12e-263\n 3 humidity                              -123.      24.6      -4.98 6.44e-  7\n 4 seasons_Spring                         -53.1      6.50     -8.17 3.64e- 16\n 5 seasons_Summer                         247.      19.1      12.9  8.17e- 38\n 6 seasons_Winter                        -290.      20.8     -14.0  1.08e- 43\n 7 holiday_No.Holiday                      20.8      5.55      3.75 1.81e-  4\n 8 day_type_Weekend                       -43.7      5.02     -8.70 4.08e- 18\n 9 seasons_Spring_x_holiday_No.Holiday    -18.9      6.76     -2.80 5.14e-  3\n10 seasons_Summer_x_holiday_No.Holiday    -13.3      7.58     -1.75 8.00e-  2\n# ℹ 19 more rows\n\n\n\nLASSO_final |&gt;\n  fit(bike_train) |&gt;\n  tidy()\n\n# A tibble: 15 × 3\n   term               estimate      penalty\n   &lt;chr&gt;                 &lt;dbl&gt;        &lt;dbl&gt;\n 1 (Intercept)          731.   0.0000000001\n 2 hour                 196.   0.0000000001\n 3 temp                 204.   0.0000000001\n 4 humidity            -233.   0.0000000001\n 5 wind_speed            13.6  0.0000000001\n 6 vis                    6.40 0.0000000001\n 7 dew_point_temp       156.   0.0000000001\n 8 solar_radiation      -68.4  0.0000000001\n 9 rainfall             -68.4  0.0000000001\n10 snowfall              15.2  0.0000000001\n11 seasons_Spring       -54.5  0.0000000001\n12 seasons_Summer       -75.4  0.0000000001\n13 seasons_Winter      -151.   0.0000000001\n14 holiday_No.Holiday    29.2  0.0000000001\n15 day_type_Weekend     -35.3  0.0000000001\n\n\n– For the regression tree model, give a plot of the final fit\n\ntree_final_fit |&gt;\n  extract_fit_engine() |&gt;\n  rpart.plot::rpart.plot(roundint = FALSE)\n\nWarning: labs do not fit even at cex 0.15, there may be some overplotting\n\n\n\n\n\n\n\n\n\n– For the bagged tree and random forest models, produce a variable importance plot\n\n# Bagged tree\nbag_final_model &lt;- extract_fit_engine(bag_final_fit)\nbag_final_model$imp |&gt;\n  mutate(term = factor(term, levels = term)) |&gt;\n  ggplot(aes(x = term, y = value)) +\n  geom_bar(stat = \"identity\") +\n  coord_flip()\n\n\n\n\n\n\n\n\nWe see that the most important predictor for our bagged model is the temperature predictor.\n\n# Random Forest\nrf_final_model &lt;- extract_fit_engine(rf_final_fit)\n#rf_final_model$imp |&gt;\n#  mutate(term = factor(term, levels = term)) |&gt;\n#  ggplot(aes(x = term, y = value)) +\n#  geom_bar(stat = \"identity\") +\n#  coord_flip()"
  },
  {
    "objectID": "Seoul Bike Data Model Fitting.html#fit-random-forest-model-to-the-entire-data-set",
    "href": "Seoul Bike Data Model Fitting.html#fit-random-forest-model-to-the-entire-data-set",
    "title": "Seoul Bike Data Modeling",
    "section": "Fit Random Forest Model to the Entire Data Set",
    "text": "Fit Random Forest Model to the Entire Data Set\n\nrf_full_fit &lt;- rf_final_wkf |&gt;\n  fit(bike_data)\nrf_full_fit\n\n══ Workflow [trained] ══════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: rand_forest()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n5 Recipe Steps\n\n• step_date()\n• step_mutate()\n• step_rm()\n• step_dummy()\n• step_normalize()\n\n── Model ───────────────────────────────────────────────────────────────────────\nRanger result\n\nCall:\n ranger::ranger(x = maybe_data_frame(x), y = y, mtry = min_cols(~11L,      x), num.threads = 1, verbose = FALSE, seed = sample.int(10^5,      1)) \n\nType:                             Regression \nNumber of trees:                  500 \nSample size:                      8465 \nNumber of independent variables:  14 \nMtry:                             11 \nTarget node size:                 5 \nVariable importance mode:         none \nSplitrule:                        variance \nOOB prediction error (MSE):       29576.02 \nR squared (OOB):                  0.9283205"
  }
]